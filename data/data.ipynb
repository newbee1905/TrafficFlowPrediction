{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "\n",
    "We will use numpy, pandas and sklearn scales and data splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Scaling Data\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Splitting data to train and test\n",
    "# Cause we only have one data file\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data: str, lags: int) -> (np.ndarray, np.ndarray, np.ndarray, StandardScaler):\n",
    "    \"\"\"\n",
    "    Process Data\n",
    "    Reshape and split data into train and test data.\n",
    "    \n",
    "    Parameters:\n",
    "        data (str): name of the excel data file\n",
    "        lags (int): time lag\n",
    "    \n",
    "    Returns:\n",
    "        X_train (np.ndarray)\n",
    "        y_train (np.ndarray)\n",
    "        X_test (np.ndarray)\n",
    "        y_test (np.ndarray)\n",
    "        flow_scaler (MinMaxScaler)\n",
    "        latlong_scaler (MinMaxScaler)\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(data, sheet_name=\"Data\", header=[1])\n",
    "\n",
    "    first_velo_col_pos = df.columns.get_loc(\"V00\")\n",
    "    flow_data = df.to_numpy()[:, first_velo_col_pos:]\n",
    "\n",
    "    flow_scaler = MinMaxScaler(feature_range=(0, 1)).fit(flow_data)\n",
    "    flow_values = flow_scaler.transform(flow_data)\n",
    "\n",
    "    lat_data = df['NB_LATITUDE'].to_numpy().reshape(-1, 1)\n",
    "    long_data = df['NB_LONGITUDE'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    latlong_data = np.concatenate((lat_data, long_data), axis=1)\n",
    "\n",
    "    latlong_scaler = MinMaxScaler(feature_range=(0, 1)).fit(latlong_data)\n",
    "\n",
    "    latlong = latlong_scaler.transform(latlong_data)\n",
    "\n",
    "    num_time_steps = 96\n",
    "    #\n",
    "    # 15 minutes per velo\n",
    "    time_values = np.arange(num_time_steps) * 15 / 24 / 60\n",
    "    time_column = np.tile(time_values, len(flow_values)).reshape(-1, 1)\n",
    "\n",
    "    expanded_latlong = np.repeat(latlong, num_time_steps, axis=0).reshape(-1, 2)\n",
    "\n",
    "    shifted_flow_values = np.roll(flow_values, -1, axis=1)\n",
    "    shifted_flow_column = shifted_flow_values.reshape(-1, 1)\n",
    "    \n",
    "    train = np.hstack((time_column, expanded_latlong, shifted_flow_column))\n",
    "\n",
    "    np.random.shuffle(train)\n",
    "\n",
    "    X = train[:, :-1]\n",
    "    y = train[:, -1]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size = .75)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, flow_scaler, latlong_scaler"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
